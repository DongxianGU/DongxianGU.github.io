<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.1">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"yoursite.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":"flat"},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="知识图谱相关知识">
<meta property="og:type" content="article">
<meta property="og:title" content="知识图谱">
<meta property="og:url" content="http://yoursite.com/2020/05/08/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1/index.html">
<meta property="og:site_name" content="Summer So Cold">
<meta property="og:description" content="知识图谱相关知识">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2020-05-08T03:12:01.000Z">
<meta property="article:modified_time" content="2020-05-11T08:09:32.942Z">
<meta property="article:author" content="DongxianGu">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://yoursite.com/2020/05/08/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>知识图谱 | Summer So Cold</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Summer So Cold</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/05/08/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="DongxianGu">
      <meta itemprop="description" content="Life,Happy,Code">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Summer So Cold">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          知识图谱
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-05-08 11:12:01" itemprop="dateCreated datePublished" datetime="2020-05-08T11:12:01+08:00">2020-05-08</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-05-11 16:09:32" itemprop="dateModified" datetime="2020-05-11T16:09:32+08:00">2020-05-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/NLP/" itemprop="url" rel="index"><span itemprop="name">NLP</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2020/05/08/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2020/05/08/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1/" itemprop="commentCount"></span>
    </a>
  </span>
  
  
            <div class="post-description">知识图谱相关知识</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="知识图谱"><a href="#知识图谱" class="headerlink" title="知识图谱"></a>知识图谱</h1><h2 id="知识表示学习"><a href="#知识表示学习" class="headerlink" title="知识表示学习"></a>知识表示学习</h2><p>表示学习：将<strong>研究对象</strong>的语义信息表示为稠密低维的实值向量。</p>
<p>研究对象：文字（词汇、短语、句子、文章）、图片、语音等</p>
<p>知识表示学习：将知识库中的实体和关系表示为稠密低维的实值向量。</p>
<p>知识图谱中包含实体和关系，其中节点表示实体，连边代表关系。</p>
<p>知识通常用三元组表示：（head,relation,tail）</p>
<p>知识表示学习的意义</p>
<ul>
<li>低维向量提高计算效率</li>
<li>稠密向量缓解数据稀疏</li>
<li>多源的异质信息表示形式统一，便于迁移和融合。</li>
</ul>
<h3 id="知识表示学习的代表模型"><a href="#知识表示学习的代表模型" class="headerlink" title="知识表示学习的代表模型"></a>知识表示学习的代表模型</h3><h3 id="基于距离的模型"><a href="#基于距离的模型" class="headerlink" title="基于距离的模型"></a>基于距离的模型</h3><ol>
<li><p>基于距离的模型——UM</p>
<p>利用头实体和尾实体的共现信息，忽略它们之间的关系。</p>
<p>得分函数为：</p>
<script type="math/tex; mode=display">
f(h,r,t) = -||h-t||_{l1/l2}</script><p>Bordes A, Glorot X, Weston J, et al. Joint learning of words and meaning representations for open-text semantic parsing. Artificial Intelligence and Statistics. 2012: 127-135.</p>
</li>
<li><p>基于距离的模型——SE</p>
<p>和UM比较，SE添加了关系信息：将关系建模为分别针对头实体和尾实体的矩阵，得分函数为：</p>
<script type="math/tex; mode=display">
f(h,r,t) = - ||M_r^hh-M_r^tt||_{l1/l2}</script><p>Bordes A, Weston J, Collobert R, et al. Learning structured embeddings of knowledge bases. AAAI. 2011.</p>
</li>
</ol>
<h3 id="基于翻译的模型"><a href="#基于翻译的模型" class="headerlink" title="基于翻译的模型"></a>基于翻译的模型</h3><ol>
<li><p>基于翻译的模型——TransE</p>
<p>将每条知识(head,relation,tail)中的relation看成从head到tail的翻译操作。</p>
<script type="math/tex; mode=display">
f(h,r,t) = ||h+r-t||_{l1/l2}</script><p>例如：</p>
<p>（北京，是…的首都，中国）</p>
<p>（华盛顿，是…的首都，美国）</p>
</li>
</ol>
<script type="math/tex; mode=display">
   L=\sum_{\xi \in T}\sum_{\xi' \in T'} [\gamma + f(\xi) - f(\xi')]_{+}</script><blockquote>
<p>我的理解，这个损失函数应该采用了负采样，$\xi$表示正例，$\xi’$表示负例，求损失函数的最小值。</p>
</blockquote>
<p>   TransE的缺点：无法处理复杂关系，比如1-N，N-1,N-N关系。</p>
<p>   例如：（奥巴马，是…总统，美国）、（布什，是…总统，美国）,在TransE中，奥巴马和布什的向量会变的相同。</p>
<p>   Bordes A, Usunier N, Garcia-Duran A, et al. Translating embeddings for modeling multi-relational data. NIPS2013: 2787-2795</p>
<ol>
<li><p>基于翻译的模型——TransH</p>
<p>为了解决TransE的问题，TransH将每种关系建模为一个超平面，将三元组中的头实体和尾实体分别映射到超平面上。</p>
<p>得分函数为：</p>
<script type="math/tex; mode=display">
\begin{align}
& h_{\perp} = h-w_r^Thw_r,\quad t_{\perp} = t - w_r^Ttw_r \\
& f(h,r,t) = - ||h_{\perp}+r-t_{\perp}||_{l1/l2}
\end{align}</script><p>Wang Z, Zhang J, Feng J, et al. Knowledge graph embedding by translating on hyperplanes. AAAI2014</p>
</li>
<li><p>基于翻译的模型——TransR/CTransR</p>
<p>TransE和TransH将实体和关系嵌入到同一个向量空间。</p>
<p>TransR表示不同的关系应该嵌入不同的语义空间，将关系嵌入为矩阵，将实体映射到关系子空间中</p>
<p>CTransR将属于同一种关系的头尾实体对分成聚成多个类，针对每个类学习不同的关系矩阵。</p>
<script type="math/tex; mode=display">h_{\perp} = hM_r,\quad t_{\perp} = tM_r</script><p>Lin Y, Liu Z, Sun M, et al. Learning entity and relation embeddings for knowledge graph completion. AAAI2015.</p>
</li>
<li><p>基于翻译的模型——TransD</p>
<p>TransH和TransR/CTransR中，不同种类的实体共享相同的映射向量或矩阵，但一个关系的头尾实体的种类和属性往往具有较大差别。</p>
<p>TransR引入了空间映射，但是也产生了大量的模型参数。</p>
<p>TransD则将每个对象（实体、关系）嵌入为两个向量：语义向量、映射向量。</p>
<script type="math/tex; mode=display">
\begin{align}
& M_r^h=r_ph_p^T+I^{k*d},\quad M_r^t = r_pt_p^T+I^{k*d} \\
& h_{\perp} = M_r^hh,\quad t_{\perp} = M_r^tt
\end{align}</script><p>Ji G, He S, Xu L, et al. Knowledge graph embedding via dynamic mapping matrix. ACL2015: 687-696</p>
</li>
<li><p>基于翻译的模型——TranSparse</p>
<p>为了克服关系的<strong>异质性</strong>（有的关系连接的头尾实体对较多，有的较少）和<strong>不平衡性</strong>（同一关系的头实体和尾实体的数量不对称），TranSparse（share/separate）用自适应的稀疏矩阵代替一般的映射矩阵，稀疏度由关系连接的头尾实体对的数量决定。</p>
<script type="math/tex; mode=display">
\begin{align}
& h_{\perp} = M_r(\theta_r)h,\quad t_{\perp} = M_r(\theta_r)t \\
& h_{\perp} = M_r^h(\theta_r^h)h,\quad t_{\perp} = M_r^t(\theta_r^t)t
\end{align}</script><p>Ji G, Liu K, He S, et al. Knowledge graph completion with adaptive sparse transfer matrix. AAAI. 2016</p>
</li>
<li><p>基于翻译的模型——TransM</p>
<p>TransM对前面几种模型的基础条件进行了放宽，即$h+r \approx t$。方法是在前面得分函数的基础上增加了权重：</p>
<script type="math/tex; mode=display">
w_r = \frac{1}{\log (h_rpt_r + t_rph_r)}</script><p>Fan M, Zhou Q, Chang E, et al. Transition-based knowledge graph embedding with relational mapping properties. Proceedings of the 28th Pacific Asia Conference on Language, Information and Computing. 2014</p>
</li>
<li><p>基于翻译的模型——ManiFoldE</p>
<p>ManiFoldE将约束$h+r \approx t$放宽为一种基本流形的约束，得分函数为：</p>
<script type="math/tex; mode=display">
\begin{align}
& f(h,r,t) = ||M(h,r,t) - D_r^2||_{l1/l2} \\
& M(h,r,t) = |||h+r-t|_{l2}
\end{align}</script><p>$M()$是一个流形，以(h,r,<em>)为例，所有合适的尾实体都分布在高维流形上，例如$M()$是一个高维球体，那么所有的\</em>都在以$h+r$为球心，以$D_r$为半径的球面上。</p>
<p>Xiao H, Huang M, Zhu X. From one point to a manifold: Knowledge graph embedding for precise link prediction. arXiv preprint arXiv:1512.04792, 2015</p>
</li>
<li><p>基于翻译的模型——TransF</p>
<p>TransF将约束$h+r \approx t$放宽为：只要求$h+r \;or\;t-r$的方向与$t\;or\;h$一致。得分函数同时衡量了$h+r$和$t$,$t-r$和$h$的方向：</p>
<script type="math/tex; mode=display">
f(h,r,t) = (h+r)^Tt+(t-r)^Th</script><p>Feng J, Huang M, Wang M, et al. Knowledge graph embedding by flexible translation. Fifteenth International Conference on the Principles of Knowledge Representation and Reasoning. 2016.</p>
</li>
<li><p>基于翻译的模型——TransA</p>
<p>TransE及其扩展模型的两个问题：</p>
<ul>
<li>得分函数采用L1或L2距离，灵活度不够</li>
<li>损失函数过于简单，实体和关系向量的每一维等同考虑。</li>
</ul>
<p>TransA将损失函数中的距离度量改成马式距离，并为每一维学习不同的权重，$W_r$是非负对矩阵。</p>
<script type="math/tex; mode=display">
f(h,r,t) = (h+r-t)^TW_r(h+r-t)</script><blockquote>
<p>看起来像二次型那块的公式</p>
</blockquote>
<p>Xiao H, Huang M, Hao Y, et al. TransA: An adaptive approach for knowledge graph embedding. arXiv preprint arXiv:1509.05490, 2015</p>
</li>
<li><p>基于翻译的模型——KG2E</p>
<p>KG2E使用高斯分布来表示实体和关系。高斯分布的均值表示实体或关系在语义空间的中心位置，而高斯分布的协方差则表示实体或关系的不确定度。</p>
<ul>
<li>头尾实体间的关系的概率分布：<script type="math/tex; mode=display">
P_e \sim N(h-t,\sum_h+\sum_t)</script></li>
</ul>
</li>
</ol>
<ul>
<li><p>实际关系的概率分布</p>
<script type="math/tex; mode=display">
    P_r \sim N(r,\sum_r)</script><ul>
<li><p>损失函数：KL散度/期望概率(计算上述两个概率的相似度)</p>
<p><a href="https://www.jianshu.com/p/43318a3dc715?isappinstalled=0" target="_blank" rel="noopener">https://www.jianshu.com/p/43318a3dc715?isappinstalled=0</a></p>
<script type="math/tex; mode=display">
    D_{KL}(P_e||P_r) = \sum_{i=1}^N P_e(x_i)\cdot(\log P_e(x_i)-\log P_r(x_i))</script><p>  He S, Liu K, Ji G, et al. Learning to represent knowledge graphs with gaussian embedding. Proceedings of the 24th ACM International on Conference on Information and Knowledge Management. ACM, 2015: 623-632.</p>
</li>
</ul>
</li>
</ul>
<ol>
<li><p>基于翻译的模型——TransG</p>
<p>也使用高斯分布来表示实体和关系，但认为一个关系连接不同的头尾实体时拥有不同的语义。所以关系的分布是高斯分布的混合。</p>
<script type="math/tex; mode=display">
\begin{align}
& h \sim N(\mu_h,\sigma_h^2I),\quad t \sim N(\mu_t,\sigma_t^2I) \\
& r = \sum_i \pi_r^i\mu_r^i,\quad \mu_r^i \sim N(\mu_h-\mu_t,(\sigma_h^2+\sigma_t^2)I)
\end{align}</script><p>得分函数：</p>
<script type="math/tex; mode=display">
f(h,r,t) = \sum_i\pi_r^i \exp(\frac{-||\mu_h+\mu_r^i-\mu_t||_{l2}}{\sigma_h^2+\sigma_t^2})</script><p>Xiao H, Huang M, Zhu X. TransG: A generative model for knowledge graph embedding. Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2016, 1: 2316-2325.</p>
</li>
</ol>
<h2 id="语义匹配模型"><a href="#语义匹配模型" class="headerlink" title="语义匹配模型"></a>语义匹配模型</h2><ol>
<li><p>LFM （Latent Factor Model）</p>
<p>LFM利用基于关系的双线性变换，刻画实体和关系之间的二阶联系。协同性较好，计算复杂度低，得分函数：</p>
<script type="math/tex; mode=display">
f(h,r,t) = h^TM_rt</script><p>Jenatton R, Roux N L, Bordes A, et al. A latent factor model for highly multirelational data. NIPS. 2012: 3167-3175.</p>
</li>
<li><p>DistMult</p>
<p>DistMult将LFM中关系的表示矩阵限制为对角阵，极大降低了模型复杂度，模型效果反而得到显著提升。</p>
<script type="math/tex; mode=display">
f(h,r,t) = h^Tdiag(M_r)t</script><p>Yang B, Yih W, He X, et al. Embedding entities and relations for learning and inference in knowledge bases. arXiv preprint arXiv:1412.6575, 2014</p>
</li>
<li><p>ComplEx</p>
<p>为了更好的对非对称关系建模，ComplEx将DistMult中实体和关系的表示扩展到复数向量空间中，即向量的每一维都是复数，而不在是实数。    得分函数如下，$Re(x)$是取x的实部。</p>
<script type="math/tex; mode=display">
f(h,r,t) = Re(h^Tdiag(M_r)t)</script><p>Trouillon T, Welbl J, Riedel S, et al. Complex embeddings for simple link prediction. International Conference on Machine Learning. 2016: 2071-208</p>
</li>
<li><p>ANALOGY</p>
<p>ANALOGY对知识图谱中的类比关系进行建模。例如：太阳系中的太阳之于行星，原子系统中的原子核之于核外电子。</p>
<p>得分函数同LFM:</p>
<script type="math/tex; mode=display">
f(h,r,t) = h^TM_rt</script><p>约束条件：</p>
<ul>
<li>正规性：$M_rM_r^T=M_r^TM_r$，如：对称矩阵可以对对称关系建模（好友关系），负对称矩阵对非对称关系建模（父子关系）</li>
<li>交换性：$M<em>rM</em>{r’}=M<em>{r’}M</em>{r}$</li>
</ul>
<p>Liu H, Wu Y, Yang Y. Analogical inference for multi-relational embeddings. Proceedings of the 34th International Conference on Machine Learning-Volume JMLR. org, 2017: 2168-2178.</p>
</li>
<li><p>RESCAL</p>
<p>RESCAL是矩阵分解模型的典型代表。在该模型中，知识图谱中的所有实体对和所有关系构成一个张量$\mathbf{X}$，如果三元组（h,r,t）存在，则$X_{hrt}=1$，否则为0。</p>
<p>张量分解旨在将每个三元组（h,r,t）分解为实体和关系表示$X<em>{hrt}=h^tM_rt$，使得$X</em>{hrt}$尽量接近1。</p>
<p>RESCAL的基本思想和LFM类似，不同的是，RESCAL会优化张量中的所有位置，包括值为0的位置，而LFM只优化知识图谱中存在的三元组。</p>
<p>Nickel M, Tresp V, Kriegel H P. A Three-Way Model for Collective Learning on Multi-Relational Data. ICML. 2011, 11: 809-816.</p>
</li>
<li><p>HolE</p>
<p>HolE定义了一种循环相关的操作，更好的捕捉实体间的语义关联。</p>
<ul>
<li>循环相关操作定义：<script type="math/tex; mode=display">
[h\otimes t]_k = \sum_{i=0}^{d-1} [h]_i*[t]_{(i+k)\mod d}</script></li>
</ul>
</li>
</ol>
<ul>
<li>得分函数：<script type="math/tex; mode=display">
   f(h,r,t) = r^T((h\otimes t) = \sum_{k=0}^{d-1}([r]_k\sum_{i=0}^{d-1} [h]_i*[t]_{(i+k)\mod d}))</script></li>
</ul>
<p>Nickel M, Rosasco L, Poggio T. Holographic embeddings of knowledge graphs. AAAI. 2016.</p>
<ol>
<li><p>SLM</p>
<p>SLM利用标准非线性单层神经网络来连接尸体</p>
<p>得分函数：</p>
<script type="math/tex; mode=display">
f(h,r,t) = u_r^T \tanh(M_r^hh+M_r^tt)</script><p>Socher R, Chen D, Manning C D, et al. Reasoning with neural tensor networks for knowledge base completion. NIPS. 2013: 926-93</p>
</li>
<li><p>SME</p>
<p>SME利用神经网络结构，为三元组设计了两种得分函数：</p>
<ul>
<li>线性形式：<script type="math/tex; mode=display">
f(h,r,t) = (M_1h+M_2r+b_1)^T(M_3t+M_4r+b_2)</script></li>
</ul>
</li>
</ol>
<ul>
<li>双线性形式：<script type="math/tex; mode=display">
   f(h,r,t) = (M_1h\otimes M_2r+b_1)^T(M_3t\otimes M_4r+b_2)</script></li>
</ul>
<p>Bordes A, Glorot X, Weston J, et al. A semantic matching energy function for learning with multi-relational data. Machine Learning, 2014, 94(2): 233-259.</p>
<ol>
<li><p>NTN</p>
<p>NTN用张量网络捕获头尾实体间的语义关联，得分函数：</p>
<script type="math/tex; mode=display">
f(h,r,t) = u_r^T \tanh \bigg(h^TM_rt+V_r \tbinom{h}{t}+b_r\bigg)</script><p>$u_r$是与关系相关的线性层，$M_r$是所有关系共享的三阶张量，$V_r$是与关系相关的投影举证。</p>
<p>Socher R, Chen D, Manning C D, et al. Reasoning with neural tensor networks for knowledge base completion. NIPS. 2013: 926-934.</p>
</li>
<li><p>MLP</p>
<p>MLP使用标准的多层神经网络捕获头尾实体间的语义关联，得分函数：</p>
<script type="math/tex; mode=display">
f(h,r,t) = w_r^T \tanh(M_r\cdot(h,r,t))</script><p>$M_r$是与关系相关的第一层神经网络的权重，$w_r$是与关系相关的第二层神经网络的权重，二者为所有关系共享。</p>
<p>Dong X, Gabrilovich E, Heitz G, et al. Knowledge vault: A web-scale approach to probabilistic knowledge fusion. Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 2014: 601-610.</p>
</li>
<li><p>NAM</p>
<p>NAM使用1+L层DNN捕获头尾实体间的语义关联：</p>
<script type="math/tex; mode=display">
\begin{align}
& a^{(l)} =W^{(l)}z^{(l-1)} + b^{(l)},\quad l=1,\dots,L \\
& z^{(l)} = ReLU(a^{(l)}),\quad l=1,\dots,L
\end{align}</script><p>$(h;r)$作为输入，即$z^0$</p>
<p>得分函数：</p>
<script type="math/tex; mode=display">
f(h,r,t) = \sigma(z^{(L)}\cdot t)</script><p>NAM为复杂关系设计了神经网络</p>
<p>RMNN（relation-modulated neural networks）:</p>
<script type="math/tex; mode=display">
\begin{align}
& a^{(l)} = W^{(l)}z^{(l-1)}+B^{(l)}r,\quad l=1,\dots,L \\
& z^{(l)} = ReLU(a^{(l)}),l=1,\dots,L
\end{align}</script><p>$h$作为输入，即$z^0$</p>
<p>得分函数：</p>
<script type="math/tex; mode=display">
f(h,r,t) = \sigma(z^{(L)} \cdot t + B^{(L+1)}r)</script></li>
</ol>
<pre><code>Liu Q, Jiang H, Evdokimov A, et al. Probabilistic reasoning via deep learning: Neural association models. arXiv preprint arXiv:1603.07704, 2016.
</code></pre><ol>
<li><p>ConvE</p>
<p>ConvE利用卷积神经网络捕获实体间的语义关联：</p>
<script type="math/tex; mode=display">
f(h,r,t) = \sigma((vec([\bar{h};\bar r]\cdot w) W)t)</script><p>$w$是卷积核(多个)，$W$是映射矩阵</p>
<p>Dettmers T, Minervini P, Stenetorp P, et al. Convolutional 2d knowledge graph embeddings. AAAI. 2018.</p>
</li>
</ol>
<h3 id="融合多源信息的模型"><a href="#融合多源信息的模型" class="headerlink" title="融合多源信息的模型"></a>融合多源信息的模型</h3><p>实体类别信息：SSE、TKRL</p>
<ul>
<li>Guo S, Wang Q, Wang B, et al. Semantically smooth knowledge graph embedding. Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2015, 1: 84-94.</li>
<li>Xie R, Liu Z, Sun M. Representation Learning of Knowledge Graphs with Hierarchical Types. IJCAI. 2016: 2965-2971.</li>
</ul>
<p>关系路径</p>
<p>PTransE 在 TransE的基础上加入了实体对间的路径信息，得分函数为：</p>
<script type="math/tex; mode=display">
\begin{align}
& f(h,r,t) = E(h,r,t) + E(h,P,t) \\
& E(h,r,t) = ||h+r-t|| \\
& E(h,P,t) = \frac{1}{Z} \sum_{p \in P(h,t)} R(p|h,t)E(h,p,t)\\
& E(h,p,t) = ||h+p-t|| = ||p-(t-h)|| = ||p-r|| = E(p,r)
\end{align}</script><p>由于实体对间的所以路径都是可靠的，因此，PtransE提出了Path-Contraint Resource Allocation图算法度量关系路径的可靠性。$R(p|h,t)$表示从h经过路径p，t所获得的资源数量。</p>
<p>Lin Y, Liu Z, Luan H, et al. Modeling relation paths for representation learning of knowledge bases. arXiv preprint arXiv:1506.00379, 2015</p>
<p>文本描述信息</p>
<p>NTN</p>
<p>Socher R, Chen D, Manning C D, et al. Reasoning with neural tensor networks for knowledge base completion. NIPS. 2013: 926-934.</p>
<p>DKRL</p>
<p>Xie R, Liu Z, Jia J, et al. Representation learning of knowledge graphs with<br>entity descriptions. AAAI. 2016</p>
<p>TEKE</p>
<p>Wang Z, Li J Z. Text-Enhanced Representation Learning for Knowledge Graph. IJCAI. 2016: 1293-1299</p>
<p>逻辑规则</p>
<p>ILP Integer Linear Programming</p>
<p>Wang Q, Wang B, Guo L. Knowledge base completion using embeddings and<br>rules. IJCAI. 2015</p>
<p>KALE</p>
<p>Guo S, Wang Q, Wang L, et al. Jointly embedding knowledge graphs and<br>logical rules. EMNLP. 2016: 192-202.</p>
<p>PUGE</p>
<p>Guo S, Wang Q, Wang L, et al. Knowledge graph embedding with iterative<br>guidance from soft rules. AAAI. 2018</p>
<p>实体属性信息</p>
<p>Nickel M, Tresp V, Kriegel H P. Factorizing yago: scalable machine learning for linked data. WWW. 2012: 271-280.</p>
<p>时序信息</p>
<p>Jiang T, Liu T, Ge T, et al. Encoding temporal information for time-aware link<br>prediction. EMNLP. 2016: 2350-2354.</p>
<p>图结构</p>
<p>GAKE</p>
<p>Feng J, Huang M, Yang Y. GAKE: graph aware knowledge embedding. COLING. 2016: 641-651</p>
<h3 id="较新成果"><a href="#较新成果" class="headerlink" title="较新成果"></a>较新成果</h3><ol>
<li><p>TransC</p>
<p>TransC将知识图谱中的实例和概念区别对待：实例嵌入为向量，概念嵌入为球体。用点和球、球和球之间的相对位置关系对instanceOf和subClassOf两种关系建模，普通关系采用TransE模型。</p>
</li>
</ol>
<script type="math/tex; mode=display">
\begin{align}
& f_e(i,c) = ||i-p||_2-m \\
& d = ||p_i-p_j||_2
\end{align}</script><p>​                    Lv X, Hou L, Li J, et al. Differentiating Concepts and Instances for Knowledge Graph Embedding.EMNLP. 2018: 1971-1979.</p>
<ol>
<li><p>TransN</p>
<p>利用实体在知识图谱中的邻居节点，将实体和关系分别嵌入为两种向量：</p>
<ul>
<li>语义向量：用于表示实体或关系的语义</li>
<li>上下文向量：用于表示其他实体或关系的上下文</li>
</ul>
<p>TransN选择邻居节点（实体关系对）</p>
<ul>
<li>利用实体的邻居总数动态计算所选邻居的数量</li>
<li>利用关系间的相似度动态计算邻居节点的权重</li>
</ul>
<p>Wang C C, Cheng P J. Translating Representations of Knowledge Graphs with Neighbors. SIGIR. 2018: 917-920</p>
</li>
<li><p>GAN-based Framework</p>
<p>利用生成对抗网络中的生成器进行高质量的负采样，用鉴别器计算reward和进行表示学习。</p>
<p>Wang P, Li S, Pan R. Incorporating GAN for negative sampling in knowledge representation learning. AAAI. 2018.</p>
</li>
</ol>
<h3 id="模型评测"><a href="#模型评测" class="headerlink" title="模型评测"></a>模型评测</h3><ol>
<li><p>常用数据集</p>
<ul>
<li>WordNet是著名的词典知识库，主要用于词义消歧。主要定义了名词、动词、形容词和副词之间的语义关系。例如名词之间的上下位关系（如 猫科动物是猫的上位词）、动词之间额蕴含关系（如 打鼾蕴含睡眠）等。</li>
<li>Freebase将WordNet和Wikipedia进行了结合，用WordNet的本体知识补充Wikipedia中实体的上下位知识。其数据基于RDF三元组模型，底层采用图数据库进行存储。</li>
<li>YAGO是一个综合型知识库，整合了Wikipedia、WordNet和GeoNames等数据源</li>
<li>其他常用数据集，WN11,WN18,FB13,FB15K,FB1M,FB5M</li>
</ul>
</li>
<li><p>链接预测</p>
<p>指预测含有普通关系的三元组中丢失的头实体或尾实体。预测结果是候选实例根据损失值排名的清单，而不直接给出最匹配实例。</p>
<p>评测标准一般两种：</p>
<ul>
<li>MRR：所有正确实例排名的倒数的平均值;</li>
<li>Hists@N：正确实例的排名中不大于N的比例</li>
</ul>
</li>
<li><p>三元组分类</p>
<p>知识图谱中的三元组分类表示一个二分类问题，其是判断给定的三元组是否是知识图谱中真实存在的。</p>
<p>评测标准：Accuracy、Precision、Recall和F1</p>
</li>
</ol>
<h3 id="知识表示学习应用"><a href="#知识表示学习应用" class="headerlink" title="知识表示学习应用"></a>知识表示学习应用</h3><ol>
<li><p>知识融合</p>
<p>Sun Z., Hu W., Li C. Cross-lingual Entity Alignment via Joint Attribute-Preserving EmbeddingISWC2017</p>
</li>
<li><p>人机交互</p>
<p>Commonsense Knowledge Aware Conversation Generation with Graph<br>Attention. IJCAI 2018</p>
</li>
</ol>
<h2 id="知识抽取"><a href="#知识抽取" class="headerlink" title="知识抽取"></a>知识抽取</h2><h3 id="问题和方法"><a href="#问题和方法" class="headerlink" title="问题和方法"></a>问题和方法</h3><p>场景（数据源）</p>
<ul>
<li>（半）结构化文本数据：百科知识中的Inforbox、规范的表格、数据库、社交网络等</li>
<li>非结构化文本数据：网页、新闻、社交媒体、论文等</li>
<li>多媒体数据：图片、视频</li>
</ul>
<p>信息抽取和知识抽取</p>
<ul>
<li>区别：信息抽取获得结构化数据，知识抽取获得机器可理解和处理的知识（知识表示）</li>
<li>关系：知识抽取建立在信息抽取的基础上，都普遍用到自然语言处理技术、基于规则的包装器和机器学习等技术。</li>
</ul>
<p>知识抽取挑战</p>
<ul>
<li>知识的不明确性 ambiguous</li>
<li>知识的不完备性 incomplete 关系、标签/属性、实体等缺失</li>
<li>知识的不一致性 inconsistent</li>
</ul>
<h3 id="知识抽取场景和方法"><a href="#知识抽取场景和方法" class="headerlink" title="知识抽取场景和方法"></a>知识抽取场景和方法</h3><ol>
<li><p>从关系数据库中抽取知识</p>
<p>抽取原理</p>
<ul>
<li>表 Table ——类 Class</li>
<li>列 Column ——属性 Property</li>
<li>行 Row —— 资源/实例 Resource/Instance</li>
<li>单元 Cell ——属性值 Property Value</li>
<li>外键 Foreign Key ——指代 Reference</li>
</ul>
<p>根据上面的规则将关系性数据库转化为一个知识库</p>
<p>抽取标准</p>
<ul>
<li>Direct Mapping</li>
<li>R2RML</li>
</ul>
<p>抽取工具</p>
<ul>
<li>D2R,Virtuoso,Orcle SW, Morph ….</li>
</ul>
<p>R2RML映射语言</p>
<ul>
<li>输入：数据库表、视图、SQL查询</li>
<li>输出：三元组</li>
</ul>
<p>一共四个步骤：抽取类-&gt;抽取属性-&gt;抽取实例-&gt;建立类之间的关系</p>
<p>优点：转换规则简单，易于实现</p>
<p>缺点：</p>
<ul>
<li>直接转换得到的知识库语义信息不足</li>
<li>需要熟悉原数据库设计的专家辅助进行知识库的优化</li>
</ul>
</li>
<li><p>百科知识的抽取</p>
<ul>
<li>大规模多语言百科知识图谱，维基百科的结构化版本，linked data核心数据集</li>
<li>覆盖127中语言，两千八百万个实体，数亿三元组，支持数据集的完全下载</li>
<li>固定模式对实体信息进行抽取，包括abstract、infobox、category、page link等</li>
</ul>
</li>
<li><p>无结构化数据的知识抽取</p>
<p>问题：当前知识图谱构建的技术瓶颈</p>
<p>关键技术：实体识别、关系抽取、事件抽取</p>
<ul>
<li><p>实体识别：抽取文本中的原子信息，例如人名、组织/机构、地理位置、时间/日期、字符、金额等等</p>
</li>
<li><p>关系抽取：抽取实体间的语义关系</p>
</li>
<li>事件抽取<ul>
<li>事件定义：具有时间、地点、参与者等基本元素，可由某个动作触发或状态改变而发生的一个图结构知识片段</li>
<li>从数据中抽取事件信息，并以结构化和语义化形式展现，例如事件发生的时间、地点、原因、参与者等。</li>
</ul>
</li>
</ul>
</li>
</ol>
<h2 id="数据获取"><a href="#数据获取" class="headerlink" title="数据获取"></a>数据获取</h2><p>爬虫的原理：</p>
<ol>
<li>获取目标数据的URL；</li>
<li>向对应URL提交HTTP请求；</li>
<li>解析HTTP响应；</li>
<li>存储解析结果。</li>
</ol>
<p>请求和响应：使用Python requests</p>
<ol>
<li>向目标URL发出不同种类的HTTP请求（GET、POST、DELETE的等）；</li>
<li>定制HTTP请求的头部，设置User-Agent和Cookie；</li>
<li>使用代理（proxy）进行请求；</li>
<li>对HTTP响应进行解析，获取状态码和文本字段。</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">url = <span class="string">'http://www.baidu.com'</span></span><br><span class="line">r = requests.get(url)</span><br><span class="line">print(r.status_code)</span><br><span class="line">print(r.text)</span><br></pre></td></tr></table></figure>
<p>数据解析</p>
<ul>
<li>Beautiful Soup 提供对HTML文本进行解析，出了自带的解析库，还支持使用lxml和html.parser</li>
<li>lxml是使用C语言编写的HTML和XML解析库，因此速度非常快</li>
<li>html.parser是Python自带的HTML解析器</li>
<li>正则表达式，对标签内容进行抽取</li>
</ul>
<p>多进程并发爬取</p>
<ul>
<li><p>Python内置的multiprocessing库提供了进程池类Pool，可以实现多进程并发爬取</p>
</li>
<li><p>Pool类的构造函数接收一个整型参数，代表进程池的大小，即并发工作进程的最大个数，默认值为计算机的CPU个数</p>
</li>
<li><p>Pool常用方法</p>
<ol>
<li><p><code>apply_async(func, args,callback, error_back)</code></p>
<p>将爬取进程添加到进程池中，并发知行。func维并发执行的函数名，args为func函数的参数列表，callback和error_callback是进程正常退出和异常退出时调用的回调函数。用于处理程序执行结构。</p>
</li>
<li><p><code>close()</code></p>
<p>关闭进程池，不能加入新的进程，用于所有进程都提交进入进程池里的时候。</p>
</li>
<li><p><code>join()</code></p>
<p>将主进程挂起，等待所有子进程执行完毕，close()必须在join()方法之前调用。</p>
</li>
</ol>
</li>
<li><p>使用回调函数或者try/except语句捕获子进程异常，否则子进程异常退出时不显示任何信息</p>
</li>
</ul>
<p>反爬虫机制</p>
<ul>
<li><p>大多数网站都由反爬虫机制，常见的有：</p>
<ul>
<li>验证HTTP请求头</li>
<li>基于用户行为的反爬虫机制</li>
</ul>
</li>
<li><p>反爬机制1原理</p>
<p>在每个HTTP请求的头部包含一些关于请求的附加信息，比如User-Agent、referer、cookie。绝大多数网站都会查看User-Agent字段。</p>
</li>
<li><p>反爬机制1应对</p>
<p>在发起HTTP请求，在请求头加入定制的信息。</p>
</li>
<li><p>反爬机制2原理</p>
<ol>
<li>拒绝同IP地址短期内发起大量请求，响应码维HTTP429</li>
<li>对一段时间内网站后台日志进行分析，对访问次数异常多的IP加入“黑名单”</li>
</ol>
</li>
<li><p>反爬机制2应对</p>
<ol>
<li>应对短时间内拒绝情况，可以间隔一段时间再请求</li>
<li>使用Selenium模拟人的行为，使用浏览器请求。</li>
<li>最方便的方式使用proxy server对目标网站进行请求。</li>
</ol>
</li>
</ul>
<p>百科页面爬取</p>
<p>微软学术数据爬取</p>
<ul>
<li>通过API获取</li>
</ul>
<p>微博数据爬取</p>
<ul>
<li><p>爬取入口</p>
<p>网页端、wap端</p>
<p>对于数据全面性不高的时候，或构建语料库时，建议爬取wap端。</p>
</li>
<li><p>处理用户登陆</p>
<ul>
<li>微博需要用户登陆才能访问，网页中的cookie会自动保留登陆信息</li>
<li>方案一：用网页端微博获取cookies数据，传入爬虫数据。</li>
<li>方案二：编写自动登陆脚本，自动获取cookies（由大量账号时）</li>
<li>小规模爬取，手动传入cookies比较方便</li>
</ul>
</li>
</ul>
<p>GitHun数据获取</p>
<ul>
<li>方案一：直接对网站页面进行请求，但是容易倍网站拒绝 429</li>
<li>方案二：使用GitHub提供的REST API和GRAPH API请求数据</li>
<li>方案三：GHTorrent项目</li>
</ul>
<h2 id="命名实体识别-Named-Entity-Recognition"><a href="#命名实体识别-Named-Entity-Recognition" class="headerlink" title="命名实体识别 Named Entity Recognition"></a>命名实体识别 Named Entity Recognition</h2><p>实体识别的任务是识别出文本中三大类命名实体：实体类、实践类、数字类。</p>
<p>序列标注体系</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">Token</th>
<th style="text-align:center">IO</th>
<th style="text-align:center">BIO</th>
<th>BIOES</th>
<th style="text-align:center">BMEWO</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">特</td>
<td style="text-align:center">I-PER</td>
<td style="text-align:center">B-PER</td>
<td>B-PER</td>
<td style="text-align:center">B-PER</td>
</tr>
<tr>
<td style="text-align:center">朗</td>
<td style="text-align:center">I-PER</td>
<td style="text-align:center">I-PER</td>
<td>I-PER</td>
<td style="text-align:center">M-PER</td>
</tr>
<tr>
<td style="text-align:center">普</td>
<td style="text-align:center">I-PER</td>
<td style="text-align:center">I-PER</td>
<td>E-PER</td>
<td style="text-align:center">E-PER</td>
</tr>
<tr>
<td style="text-align:center">在</td>
<td style="text-align:center">O</td>
<td style="text-align:center">O</td>
<td>O</td>
<td style="text-align:center">O</td>
</tr>
<tr>
<td style="text-align:center">白</td>
<td style="text-align:center">I-LOC</td>
<td style="text-align:center">B-LOC</td>
<td>B-LOC</td>
<td style="text-align:center">B-LOC</td>
</tr>
<tr>
<td style="text-align:center">宫</td>
<td style="text-align:center">I-LOC</td>
<td style="text-align:center">I-LOC</td>
<td>E-LOC</td>
<td style="text-align:center">E-LOC</td>
</tr>
<tr>
<td style="text-align:center">签</td>
<td style="text-align:center">O</td>
<td style="text-align:center">O</td>
<td>O</td>
<td style="text-align:center">O</td>
</tr>
<tr>
<td style="text-align:center">署</td>
<td style="text-align:center">O</td>
<td style="text-align:center">O</td>
<td>O</td>
<td style="text-align:center">O</td>
</tr>
</tbody>
</table>
</div>
<h3 id="基于规则和词典的实体识别"><a href="#基于规则和词典的实体识别" class="headerlink" title="基于规则和词典的实体识别"></a>基于规则和词典的实体识别</h3><p>流程</p>
<ol>
<li>预处理<ol>
<li>划分句子</li>
<li>分词+词性标注</li>
<li>构建词典</li>
</ol>
</li>
<li>识别实体边界<ol>
<li>初始化边界：词典匹配、拼写规则、特殊字符、特征词和标点符号等</li>
</ol>
</li>
<li>命名实体分类<ol>
<li>使用分类规则</li>
<li>基于词典的分类</li>
</ol>
</li>
</ol>
<p>词典的三个使用场景</p>
<ul>
<li>在分词时辅助分词</li>
<li>实体抽取时根据词典匹配实体</li>
<li>基于词典对实体分类</li>
</ul>
<p>词典的构建</p>
<blockquote>
<p>基于统计分析得到候选词典，然后在人工进行筛选，同时人工提取领域中重要的术语和复用领域现有词典。现有的综合中文语义词库包括：CSC、howne和Chinese Open Wordnet。</p>
</blockquote>
<p>词典构建统计分析方法：</p>
<ul>
<li>去停用词后统计词频，选取一定范围的名词</li>
<li>关键词抽取：TF-IDF、TextRank</li>
<li>借助维基百科页面的分类系统</li>
<li>特征词分词：词共现、特定模式</li>
<li>词性分析：从标记为人名(nh)、组织（ni）、日期（nt）等词中抽取</li>
<li>依存句法分析</li>
</ul>
<h3 id="基于机器学习的实体识别"><a href="#基于机器学习的实体识别" class="headerlink" title="基于机器学习的实体识别"></a>基于机器学习的实体识别</h3><p>主要包括</p>
<ul>
<li>隐式马尔科夫模型 Hidden Markov Model,HMM</li>
<li>最大熵马尔科夫模型 Maximum Entropy Markov Model,MEMM</li>
<li>条件随机场 Conditional Random Fields，CRF</li>
<li>支持向量机 Support Vector Machine,SVM</li>
</ul>
<ol>
<li>隐式马尔科夫模型<ul>
<li>有向图模型</li>
<li>生成模型</li>
<li>特征分布独立假设</li>
</ul>
</li>
<li>条件随机场模型<ul>
<li>无向图模型</li>
<li>判别式模型</li>
<li>无特征分布独立假设</li>
</ul>
</li>
</ol>
<h3 id="基于深度学习的实体识别"><a href="#基于深度学习的实体识别" class="headerlink" title="基于深度学习的实体识别"></a>基于深度学习的实体识别</h3><p>NN/CNN+CRF模型</p>
<p>Bi-LSTM+CRF</p>
<p>Bi-LSTM-CNN-CRF</p>
<h3 id="基于半监督学习的实体识别"><a href="#基于半监督学习的实体识别" class="headerlink" title="基于半监督学习的实体识别"></a>基于半监督学习的实体识别</h3><p>Language Model Augmented Sequence Taggers （TagLM）   Peters et al.[2017]</p>
<ol>
<li>使用海量无标注语料训练Bi-LSTM</li>
<li>获取LM embedding和Word embeding</li>
<li>将词的向量和语言模型向量混合输入到序列标注模型中进行预测</li>
</ol>
<h3 id="基于迁移学习的实体识别"><a href="#基于迁移学习的实体识别" class="headerlink" title="基于迁移学习的实体识别"></a>基于迁移学习的实体识别</h3><p>迁移学习的核心：找出新问题和原问题之间的<strong>相似性</strong>.</p>
<p>迁移学习是机器学习的一种，但也有一些区别</p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>传统机器学习</th>
<th>迁移学习</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>训练和测试数据服从相同的分布</td>
<td>训练和测试数据服从不同的分布</td>
</tr>
<tr>
<td>数据标注</td>
<td>需要足够的数据标注来训练模型</td>
<td>不需要足够的数据标注</td>
</tr>
<tr>
<td>模型</td>
<td>每个任务分别建模</td>
<td>模型可以在不同任务之间迁移</td>
</tr>
</tbody>
</table>
</div>
<p>迁移学习的三种模式：跨域、跨应用、跨语言</p>
<p>Yang et al.[2017]</p>
<h3 id="基于预训练的实体识别"><a href="#基于预训练的实体识别" class="headerlink" title="基于预训练的实体识别"></a>基于预训练的实体识别</h3><p>BERT模型：重新设计了语言模型预训练阶段的目标任务，提出了遮挡语言模型 Masked LM和下一个句子预测 NSP</p>
<ul>
<li>Masked LM：在输入的词序列中，随机选15%的词进行[MASK]，然后在15%的词中，有80%的词被真正打上[MASK]标签，10%的词随机替换成任意词汇，10%的词不做任何处理。模型的任务是去正确预测带有[MASK]标签的词，相比传统的语言模型，Masked LM可以从前后两个方向预测这些带有[Mask]标签的词</li>
<li>NSP本质上是一个二分类任务，以50%的概率输入一个句子和下一个句子的拼接，标签属于正例；另外50%的概率输入一个句子和非下一个随机句子的拼接，对应标签为负例。</li>
</ul>
<h2 id="关系抽取"><a href="#关系抽取" class="headerlink" title="关系抽取"></a>关系抽取</h2><p>语义关系</p>
<ul>
<li>指隐藏在句法结构后面由词语的语义范畴建立起来的关系</li>
<li>在句子中的地位很重要</li>
<li>连接文本中的实体</li>
<li>与实体一起表达出文本中的含义</li>
<li>并不是很难识别</li>
</ul>
<p>句法关系 Syntactic relations</p>
<ul>
<li><p>位置关系 relation  of position</p>
<ul>
<li><p>位置关系是组合关系 （Syntagmatic Relations）一个方面的表现</p>
</li>
<li><p>也被叫做横向关系或链状关系</p>
</li>
<li>Word Order<ul>
<li>SVO、VSO、SOV、OVS、OSV、VOS</li>
<li>英语就是SVO</li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li><p>替代关系 relation of substitutability</p>
<ul>
<li>指的是在某个结构的<strong>位置</strong>上彼此可以<strong>相互替换</strong>的成分之间的关系</li>
<li>The <em>__</em> smiles.<ul>
<li>比如 ：man、boy、girl</li>
<li>必须满足以下条件</li>
<li>必须有生命</li>
<li>只有人类可以</li>
<li>和smiles连接的名词必须是单数</li>
</ul>
</li>
<li>也被称为联想关系或聚合关系</li>
<li>也被称为纵聚合关系或纵向关系或选择关系</li>
</ul>
</li>
<li><p>同现关系 relation of co-occurence</p>
<ul>
<li>指的是小句子中不同集合关系的词语允许或要求和另一个集合或类别中的词语一起组成句子或句子的某一特定部分。</li>
<li>同现关系部分属于组合关系，部分属于聚合关系</li>
</ul>
</li>
</ul>
<p>组合关系 Versus 聚合关系</p>
<ul>
<li>Harris,1987 频繁出现的组合关系可能称为我们记忆中的一部分，从而称为范式</li>
<li>Gardin,1965  聚合关系的实例来源于累积的组合数据</li>
<li>反映了当前对开放文本关系抽取的思考</li>
</ul>
<p>谓词逻辑 Predicate logic [Frege,1879]</p>
<ul>
<li><p>固有关系的形式化</p>
</li>
<li><p>例如 Google buys YouTube 表示为 buy(Google,YouTube)</p>
<p>问题：</p>
<ul>
<li><p>如何处理多个参数</p>
<p>buy(Google,YouTube,for $1.65B,in 2006)</p>
</li>
<li><p>如何处理多谓词或者无谓词的情况</p>
<p>buy(Google,YouTube) ^ for($1.65B) ^ jumped(stock) ^ immediately</p>
</li>
</ul>
</li>
</ul>
<p>戴维森逻辑表示 [Davidson,1980]</p>
<ul>
<li><p>用附加的变量表示事件或者关系</p>
</li>
<li><p>事件可以被明确的修改或者量化</p>
<script type="math/tex; mode=display">
\begin{align}
& \exist e\; buy(e,Google,YouTube) \and for(e,$1.65B) \and in(e,2006) \\
& \exist e_1\; buy(e_1,Google,YouTube) \and for(e_1,$1.65B) \and \exist e_2\;jumped(e_2,stock) \and immediately(e_2) \and \exist e_3 belong-to(e_3,stock,Google)
\end{align}</script></li>
<li><p>问题</p>
<ul>
<li><p>如何处理确实或者可选的参数</p>
<p>YouTube was bought for $1.65B in 2006 没有主语</p>
</li>
</ul>
</li>
</ul>
<p>新戴维森逻辑表示</p>
<ul>
<li><p>谓词参数分解为主题角色</p>
<script type="math/tex; mode=display">
\exist e \;InstanceOfBuying(e) \and agent(e,Google) \and patient(e,YouTube)</script><script type="math/tex; mode=display">
\exist e\; InstanceOf(e,Buying) \and agent(e,Google) \and patient(e,YouTube)</script></li>
</ul>
<p>一阶逻辑的图表示</p>
<ul>
<li>Begriffsschrift(‘concepte-script’)</li>
<li>Relational graphs</li>
<li>Existential graphs</li>
<li>Conceptual graphs</li>
<li>Discourse representation structure</li>
</ul>
<p>语义关系的双重性</p>
<ul>
<li><p>逻辑方面：谓词</p>
<p>用于AI以支持基于知识的表示和推理</p>
</li>
<li><p>图方面：弧形连接概念</p>
<p>在NLP中可以表示事实性知识</p>
<p>主要是二元关系</p>
</li>
</ul>
<p>推理系统的兴起</p>
<ul>
<li>logic-based reasoning, no language</li>
<li>early NLP systems with semantic knowledge<ul>
<li>interactive English dialogue system</li>
<li>understanding children’s stories</li>
<li>conceptual shift from the “shallow” architecture of primitive conversion systems such as ELIZA</li>
</ul>
</li>
<li>large-scale hand-crafted ontologies<ul>
<li>Cyc</li>
<li>OpenMind Common Sense</li>
<li>MindPixel</li>
<li>FreeBase-truly large-scale</li>
<li>DBpedia</li>
<li>Wikidata</li>
</ul>
</li>
</ul>
<p>知识和语言的十字路口</p>
<ul>
<li>词典中的词汇可以从文本中自动学习得到</li>
<li>语义网络<ul>
<li>一种用图来表示知识的结构化方式，信息被表达为一组结点，结点通过一组带标记的有向直线彼此相连，用于表示结点之间的关系。<ul>
<li>顶点是映射文本中单词的概念</li>
<li>边代表概念间的关系</li>
</ul>
</li>
</ul>
</li>
<li>WordNet<ul>
<li>包含155000个单词 （名词、动词、形容词、副词）</li>
<li>包含十几种语义关系，例如同义词、反义词、超代名词。</li>
</ul>
</li>
</ul>
<p>自动化知识获取</p>
<ul>
<li>学习本体关系<ul>
<li>is-a</li>
<li>part-of</li>
</ul>
</li>
<li>bootstrapping</li>
<li>开放式关系抽取<ul>
<li>没有预先指定的关系列表或关系模型</li>
<li>学习关系表达的模式<ul>
<li>POS</li>
<li>paths in a syntactic tree</li>
<li>sequences of high-frequency words</li>
</ul>
</li>
<li>抽取结果很难映射到知识库中</li>
</ul>
</li>
</ul>
<p>关系抽取的用处</p>
<ol>
<li><p>构建知识库</p>
</li>
<li><p>文本分析</p>
</li>
<li><p>NLP应用</p>
<p>信息抽取、信息检索、自动摘要、机器翻译、问答、释义、文本蕴涵推理、叙词表构建、语义网络构建、词义消歧、语言建模</p>
</li>
</ol>
<h3 id="语义关系"><a href="#语义关系" class="headerlink" title="语义关系"></a>语义关系</h3><p>语义关系的两种视角</p>
<ul>
<li>概念间的关系<ul>
<li>主要是关于世界的知识</li>
<li>可以从文本中发现</li>
</ul>
</li>
<li>名词间的关系<ul>
<li>主要关注文本所表达的事件或者形势</li>
<li>可以通过知识库信息进行发现</li>
</ul>
</li>
</ul>
<p>Casagrande &amp; Hale 1967在讲外语的人实验者门得出一系列单词的定义，从中抽取了13中关系</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">Relation</th>
<th style="text-align:center">Example</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">attributive</td>
<td style="text-align:center">toad-small</td>
</tr>
<tr>
<td style="text-align:center">function</td>
<td style="text-align:center">ear-hearing</td>
</tr>
<tr>
<td style="text-align:center">operational</td>
<td style="text-align:center">shirt-wear</td>
</tr>
<tr>
<td style="text-align:center">exemplification</td>
<td style="text-align:center">circular-wheel</td>
</tr>
<tr>
<td style="text-align:center">synonymy</td>
<td style="text-align:center">thousand-ten hundred</td>
</tr>
<tr>
<td style="text-align:center">provenience</td>
<td style="text-align:center">milk-cow</td>
</tr>
<tr>
<td style="text-align:center">circularity</td>
<td style="text-align:center">X is defined as X</td>
</tr>
<tr>
<td style="text-align:center">contingency</td>
<td style="text-align:center">lightning-rain</td>
</tr>
<tr>
<td style="text-align:center">spatial</td>
<td style="text-align:center">tongue-mouth</td>
</tr>
<tr>
<td style="text-align:center">comparison</td>
<td style="text-align:center">wolf-coyote</td>
</tr>
<tr>
<td style="text-align:center">class inclusion</td>
<td style="text-align:center">bee-insect</td>
</tr>
<tr>
<td style="text-align:center">antonymy</td>
<td style="text-align:center">low-high</td>
</tr>
<tr>
<td style="text-align:center">grading</td>
<td style="text-align:center">Monday-Sunday</td>
</tr>
</tbody>
</table>
</div>
<p>Chaffin &amp; Hermann,1984 对31中语义关系的实体进行分组，最终发现五个粗粒度的类别：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">Relation</th>
<th style="text-align:center">Example</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">constrasts</td>
<td style="text-align:center">night-day</td>
</tr>
<tr>
<td style="text-align:center">similars</td>
<td style="text-align:center">car-auto</td>
</tr>
<tr>
<td style="text-align:center">class inclusion</td>
<td style="text-align:center">vehicle-car</td>
</tr>
<tr>
<td style="text-align:center">part-whole</td>
<td style="text-align:center">airplane-wing</td>
</tr>
<tr>
<td style="text-align:center">case</td>
<td style="text-align:center">relations-agent,instrument</td>
</tr>
</tbody>
</table>
</div>
<p>复合名词中的语义关系</p>
<p>复合名词</p>
<ul>
<li>定义：两个或更多名词连在一起所构成的词</li>
<li>例如：Silkworm、 healthcare reform….</li>
<li>性质:<ul>
<li>隐式关系编码：难以解读</li>
<li>丰富性：难以忽略</li>
<li>高产：无法被列入词典</li>
</ul>
</li>
</ul>
<p>复合名词是一个缩影：表征问题反映了一般语义关系</p>
<ul>
<li>语义学的大量文献 www.cl.cam.ac.uk/~do242/Resources/compound_bibliography.html</li>
<li>互补的观点<ul>
<li>语言学的：寻找最全面的解释表达</li>
<li>NLP：为特定应用选择最有用的表示方式<ul>
<li>计算易处理的</li>
<li>向下游系统提供信息输出</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>复合名词的关系能否从一个小的封闭的清单中得出的吗？</p>
<p>即是否存在一个合理小的集合，完全涵盖文本中名词短语邻近的内容。</p>
<p>布朗语料库综合研究产生的关系：</p>
<ul>
<li><p>一个四级关系层级</p>
<p>L1: Constitute</p>
<p>​    L2: Source-Result</p>
<p>​    L2: Result-Source</p>
<p>​    L2: Copula</p>
<p>​        L3: Adjective-Like_Modifier</p>
<p>​        L3:Subsumptive</p>
<p>​        L3:Attributive</p>
<p>​            L4:Animate_Head (e.g. girl firend)</p>
<p>​            L4:Inanimate_Head (e.g. house boat)</p>
</li>
<li><p>六大语义关系</p>
<p>|    Relation    |     Example     |<br>| :——————: | :——————-: |<br>|   Possession   |  family estate  |<br>|    Location    |   water polo    |<br>|    Purpose     |  water bucket   |<br>| Activity-actor | crime syndicate |<br>|  Resemblance   |   cherry bomb   |<br>|   Constitute   |    clay bird    |</p>
</li>
</ul>
<h3 id="语义关系学习"><a href="#语义关系学习" class="headerlink" title="语义关系学习"></a>语义关系学习</h3><p>方法：</p>
<ul>
<li>监督学习<ul>
<li>优点：表现很好</li>
<li>缺点：需要大量的标记数据和特征表示</li>
</ul>
</li>
<li>无监督学习<ul>
<li>优点：可扩展，适用于开放式信息提取</li>
<li>缺点：表现比较差</li>
</ul>
</li>
</ul>
<p>特征</p>
<ul>
<li><p>目的：将数据映射维向量</p>
</li>
<li><p>Entity features and relational features [Turney, 2006]</p>
</li>
<li><p>实体特征：捕获关系实体中参数语义的一些表示</p>
<ul>
<li>Basic entity feature<ul>
<li>基本实体特征包括每个候选参数的字符串值以及标记这写参数的单个单词，可能是词形化或词干化</li>
<li>例如。字符串值、单独的词、词形化或词干化</li>
<li>优点：大多数情况下，这些特征对于一个良好的关系是信息足够的</li>
<li>缺点：特征比较稀疏</li>
</ul>
</li>
<li>Background entity features<ul>
<li>句法信息，例如：语法角色</li>
<li>语义信息，例如：语义类别</li>
<li>优点：解决了数据的稀疏性问题</li>
<li>缺点：需要人工标注</li>
<li>可以使用语义类别进行聚类<ul>
<li>Brown clusters [Brown&amp;al., 1992]</li>
<li>Clustering By Committee [Pantel &amp; Lin, 2002]</li>
<li>Latent Dirichlet Allocation [Blei&amp;al., 2003]</li>
</ul>
</li>
<li>直接表示特征空间中的单词共现<ul>
<li>协调、分布表示、关系语义表示</li>
</ul>
</li>
<li>词嵌入</li>
</ul>
</li>
</ul>
</li>
<li><p>关系特征：直接对关系进行表征，表征参数间的相互作用，如对文本实体的上下文进行建模。</p>
</li>
<li><p>Basic relational features</p>
<ul>
<li>对上下文进行建模<ul>
<li>在两个参数之间的单词</li>
<li>处于参数的特定窗口或者一侧的单词</li>
<li>链接参数的依赖路径</li>
<li>一个完整的依赖图</li>
<li>最小支配子树</li>
</ul>
</li>
</ul>
</li>
<li><p>Background relational features</p>
<ul>
<li>编码关于实体通常是如何交互的知识，而不仅仅是上下文<ul>
<li>通过释义进行关系表征</li>
<li>占位符模式</li>
<li>通过聚类寻找相似上下文</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="关系抽取数据集"><a href="#关系抽取数据集" class="headerlink" title="关系抽取数据集"></a>关系抽取数据集</h3><p>语义关系学习的标注数据</p>
<ul>
<li>small-scale/large-scale</li>
<li>general-purpose/domain-specific</li>
<li>arguments marked / not marked</li>
<li>additional information about the arguments / no additional information</li>
</ul>
<p>数据：MUC和ACE</p>
<p>数据：SemEval</p>
<ul>
<li>小数量的关系</li>
<li>标注的实体</li>
<li>附加的实体信息</li>
<li>句子语境和挖掘模式</li>
</ul>
<p>SemEval-2007 Task4 SemEval-2010 Task8</p>
<p>数据：FewRel</p>
<h3 id="基于模板的实体关系抽取"><a href="#基于模板的实体关系抽取" class="headerlink" title="基于模板的实体关系抽取"></a>基于模板的实体关系抽取</h3><p>基于模板的方法：</p>
<ul>
<li>使用模式/规则挖掘关系，基于触发词/字符串等</li>
<li>基于依存句法</li>
</ul>
<p>关系挖掘模式</p>
<ul>
<li>支持大多数关系抽取系统的基本概念是关系模式<ul>
<li>它是一个表达式，当与文本片段匹配时，它能够标识出响应的关系实例</li>
</ul>
</li>
<li>例如 词典项、通配符、词性、句法关系、正则表达式等等</li>
</ul>
<p>基于依存句法</p>
<p>通常可以以动词为起点构建规则，对节点上的词性和边上的依存关系进行限定。</p>
<ol>
<li>对句子进行分词、词性标注、命名实体识别、依存分析等处理</li>
<li>根据句子依存语法树结构上匹配规则，每匹配一条规则就生成一个三元组</li>
<li>根据扩展规则对抽取到的三元组进行扩展</li>
<li>对三元组实体和触发词进一步抽取出关系</li>
</ol>
<p>例子：董卿现身国家博物馆看展优雅大方</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">词顺序</th>
<th style="text-align:center">词</th>
<th style="text-align:center">词性</th>
<th style="text-align:center">依存关系路径</th>
<th style="text-align:center">依存关系</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">0</td>
<td style="text-align:center">董卿</td>
<td style="text-align:center">人名</td>
<td style="text-align:center">1</td>
<td style="text-align:center">主语</td>
</tr>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">现身</td>
<td style="text-align:center">动词</td>
<td style="text-align:center">-1</td>
<td style="text-align:center">核心词</td>
</tr>
<tr>
<td style="text-align:center">2</td>
<td style="text-align:center">国家博物馆</td>
<td style="text-align:center">地名</td>
<td style="text-align:center">1</td>
<td style="text-align:center">宾语</td>
</tr>
<tr>
<td style="text-align:center">3</td>
<td style="text-align:center">看</td>
<td style="text-align:center">动词</td>
<td style="text-align:center">1</td>
<td style="text-align:center">顺承</td>
</tr>
<tr>
<td style="text-align:center">4</td>
<td style="text-align:center">展</td>
<td style="text-align:center">动词</td>
<td style="text-align:center">3</td>
<td style="text-align:center">补语</td>
</tr>
<tr>
<td style="text-align:center">5</td>
<td style="text-align:center">优雅</td>
<td style="text-align:center">形容词</td>
<td style="text-align:center">7</td>
<td style="text-align:center">定语</td>
</tr>
<tr>
<td style="text-align:center">6</td>
<td style="text-align:center">端庄</td>
<td style="text-align:center">形容词</td>
<td style="text-align:center">7</td>
<td style="text-align:center">定语</td>
</tr>
<tr>
<td style="text-align:center">7</td>
<td style="text-align:center">大方</td>
<td style="text-align:center">形容词</td>
<td style="text-align:center">4</td>
<td style="text-align:center">定语</td>
</tr>
</tbody>
</table>
</div>
<p>规则抽取结果</p>
<p>（董卿，现身，国家博物馆） —-&gt; 位于（董卿，国家博物馆）</p>
<ul>
<li>优点<ul>
<li>人工规则具有高准确率</li>
<li>可以为特定领域定制</li>
<li>在小规模数据集上容易实现，构建简单</li>
</ul>
</li>
<li>缺点<ul>
<li>低召回率</li>
<li>特定领域的模板需要专家构建，要考虑周全所有可能的pattern很难，也费时间尽力</li>
<li>需要为每条关系来定义pattern</li>
<li>难以维护</li>
<li>可移植性差</li>
</ul>
</li>
</ul>
<h3 id="有监督实体关系抽取"><a href="#有监督实体关系抽取" class="headerlink" title="有监督实体关系抽取"></a>有监督实体关系抽取</h3><p>关系学习的算法</p>
<ul>
<li>基于特征向量的方法<ul>
<li>从上下文信息、词性、语法等中抽取一系列特征</li>
</ul>
</li>
<li>核分类<ul>
<li>关系特征可能拥有复杂的结构</li>
</ul>
</li>
<li>序列标注方法<ul>
<li>关系中参数的跨度是可变的</li>
</ul>
</li>
</ul>
<h4 id="基于特征向量的方法"><a href="#基于特征向量的方法" class="headerlink" title="基于特征向量的方法"></a>基于特征向量的方法</h4><p>定义：从上下文信息、词性、语法等中抽取一系列特征，来训练一个分类器（朴素贝叶斯、支持向量机、最大熵等），然后完成关系抽取。</p>
<p>对于一组训练数据:</p>
<script type="math/tex; mode=display">
(x^1,y^1),(x^2,y^2),\cdots(x^n,y^n)</script><p>将二元关系抽取视为分类问题：$y^i={-1,1}$</p>
<p>进而学习得出一个分类函数</p>
<script type="math/tex; mode=display">
f = \begin{cases}
& 1 \quad存在某种关系 \\
& -1 \quad 其他情况
\end{cases}</script><h4 id="核分类"><a href="#核分类" class="headerlink" title="核分类"></a>核分类</h4><ul>
<li>观点：两个实体的相似度可以在高维的特征空间计算得到而不需要枚举特征空间的各个维度</li>
<li>convolution kernels<ul>
<li>易于对特征进行组合，例如：实体和关系</li>
</ul>
</li>
<li>kernelizable classifiers<ul>
<li>SVM、Logistic Regression、KNN、Naive Bayes</li>
</ul>
</li>
</ul>
<p>Kernels for linguistic structures</p>
<ul>
<li>string sequenceies</li>
<li>dependency paths</li>
<li>shallow parse tree</li>
<li>constituent parse trees</li>
<li>dependency parse trees</li>
<li>feature-enriched/semantic tree kernel</li>
<li>directed acyclic graphs</li>
</ul>
<h4 id="Sequential-labeling-method"><a href="#Sequential-labeling-method" class="headerlink" title="Sequential labeling method"></a>Sequential labeling method</h4><ul>
<li>能够识别句子中的实体，并且打上对应的语义类型标签，如：person、organization、location、protein等</li>
<li>对于识别出的实体，给出句子中存在的关系，如：president-of、born-in、cause、side-effect</li>
<li>HMMs / MEMMs / CRFs</li>
<li>useful for<ul>
<li>argument identification</li>
<li>relation extraction</li>
</ul>
</li>
</ul>
<ol>
<li>Sequential labeling : argument identification<ul>
<li>words: individual words,previous/following two words,word substrings,capitalization,digit patterns,manual lexicon。</li>
<li>labels: individual labels, previous/following two labels</li>
<li>combinations of words and labels</li>
</ul>
</li>
<li>relation extraction<ul>
<li>某些情况，关系抽取可以退化维序列标注问题</li>
<li>HMMs、CRFs</li>
<li>Dynamic graphical model</li>
</ul>
</li>
</ol>
<h3 id="弱监督关系抽取"><a href="#弱监督关系抽取" class="headerlink" title="弱监督关系抽取"></a>弱监督关系抽取</h3><h3 id="远程监督关系抽取"><a href="#远程监督关系抽取" class="headerlink" title="远程监督关系抽取"></a>远程监督关系抽取</h3><h3 id="无监督关系抽取"><a href="#无监督关系抽取" class="headerlink" title="无监督关系抽取"></a>无监督关系抽取</h3><h3 id="基于深度学习的关系抽取"><a href="#基于深度学习的关系抽取" class="headerlink" title="基于深度学习的关系抽取"></a>基于深度学习的关系抽取</h3><h4 id="词嵌入"><a href="#词嵌入" class="headerlink" title="词嵌入"></a>词嵌入</h4><ul>
<li>映射单词到一个实值的低维空间向量</li>
<li>做法<ul>
<li>neural networks （e.g. CBOW, skip-gram）</li>
<li>dimensionality reduction (e.g LSA,LDA,PCA)</li>
<li>explicit representation (words in the context)</li>
</ul>
</li>
<li>对于很多NLP任务都很重要</li>
</ul>
<p>Word Embeddings from a Neural LM [Bengio &amp;al.2003]</p>
<p>NNLM模型</p>
<p>Efficient Estimation of Word Representations in Vector Space [Mikolov &amp;al.2013]</p>
<p>Word2VecLinguistic Regularities in Continuous Space Word Representations [Mikolov &amp;al.2013]</p>
<p>RNNLM</p>
<p>基于句法的词嵌入</p>
<p>Linguistic Regularities in Continuous Space Word Representations [Mikolov &amp;al.2013]</p>
<p>Dependency-Based Word Embeddings [Levy&amp;Goldberg,2014]</p>
<p>Semantic Compositionality through Recursive Matrix-Vector Spaces [Socher&amp;al., 2012]</p>
<p>MV-RNN</p>
<p>Relation Classification via Convolutional Deep Neural Network [Zeng&amp;al., 2014]</p>
<p>CNN: Convolutional Deep Neural Network</p>
<p>Attention-Based Bidirectional Long Short-Term Memory Networks for Relation Classification. [Zhou, 2016, ACL]</p>
<p>RESIDE: Improving Distantly-Supervised Neural Relation Extraction using Side Information [Vashishth1, 2018, EMNLP]</p>
<p>Syntax-aware Entity Embedding for Neural Relation Extraction [He, 2018, AAAI]</p>
<p>A Hierarchical Framework for Relation Extraction with Reinforcement Learning [Takanobu, 2019, AAAI]</p>
<p>Jointly Extracting Multiple Triplets with Multilayer Translation Constraints. [Tan, 2019, AAAI]</p>

    </div>

    
    
    

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2020/05/07/CRF/" rel="prev" title="CRF">
      <i class="fa fa-chevron-left"></i> CRF
    </a></div>
      <div class="post-nav-item">
    <a href="/2020/05/11/%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96%E4%B9%8BPCNN/" rel="next" title="关系抽取之PCNN">
      关系抽取之PCNN <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="valine-comments"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#知识图谱"><span class="nav-number">1.</span> <span class="nav-text">知识图谱</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#知识表示学习"><span class="nav-number">1.1.</span> <span class="nav-text">知识表示学习</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#知识表示学习的代表模型"><span class="nav-number">1.1.1.</span> <span class="nav-text">知识表示学习的代表模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#基于距离的模型"><span class="nav-number">1.1.2.</span> <span class="nav-text">基于距离的模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#基于翻译的模型"><span class="nav-number">1.1.3.</span> <span class="nav-text">基于翻译的模型</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#语义匹配模型"><span class="nav-number">1.2.</span> <span class="nav-text">语义匹配模型</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#融合多源信息的模型"><span class="nav-number">1.2.1.</span> <span class="nav-text">融合多源信息的模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#较新成果"><span class="nav-number">1.2.2.</span> <span class="nav-text">较新成果</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#模型评测"><span class="nav-number">1.2.3.</span> <span class="nav-text">模型评测</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#知识表示学习应用"><span class="nav-number">1.2.4.</span> <span class="nav-text">知识表示学习应用</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#知识抽取"><span class="nav-number">1.3.</span> <span class="nav-text">知识抽取</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#问题和方法"><span class="nav-number">1.3.1.</span> <span class="nav-text">问题和方法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#知识抽取场景和方法"><span class="nav-number">1.3.2.</span> <span class="nav-text">知识抽取场景和方法</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#数据获取"><span class="nav-number">1.4.</span> <span class="nav-text">数据获取</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#命名实体识别-Named-Entity-Recognition"><span class="nav-number">1.5.</span> <span class="nav-text">命名实体识别 Named Entity Recognition</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#基于规则和词典的实体识别"><span class="nav-number">1.5.1.</span> <span class="nav-text">基于规则和词典的实体识别</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#基于机器学习的实体识别"><span class="nav-number">1.5.2.</span> <span class="nav-text">基于机器学习的实体识别</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#基于深度学习的实体识别"><span class="nav-number">1.5.3.</span> <span class="nav-text">基于深度学习的实体识别</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#基于半监督学习的实体识别"><span class="nav-number">1.5.4.</span> <span class="nav-text">基于半监督学习的实体识别</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#基于迁移学习的实体识别"><span class="nav-number">1.5.5.</span> <span class="nav-text">基于迁移学习的实体识别</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#基于预训练的实体识别"><span class="nav-number">1.5.6.</span> <span class="nav-text">基于预训练的实体识别</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#关系抽取"><span class="nav-number">1.6.</span> <span class="nav-text">关系抽取</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#语义关系"><span class="nav-number">1.6.1.</span> <span class="nav-text">语义关系</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#语义关系学习"><span class="nav-number">1.6.2.</span> <span class="nav-text">语义关系学习</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#关系抽取数据集"><span class="nav-number">1.6.3.</span> <span class="nav-text">关系抽取数据集</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#基于模板的实体关系抽取"><span class="nav-number">1.6.4.</span> <span class="nav-text">基于模板的实体关系抽取</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#有监督实体关系抽取"><span class="nav-number">1.6.5.</span> <span class="nav-text">有监督实体关系抽取</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#基于特征向量的方法"><span class="nav-number">1.6.5.1.</span> <span class="nav-text">基于特征向量的方法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#核分类"><span class="nav-number">1.6.5.2.</span> <span class="nav-text">核分类</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Sequential-labeling-method"><span class="nav-number">1.6.5.3.</span> <span class="nav-text">Sequential labeling method</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#弱监督关系抽取"><span class="nav-number">1.6.6.</span> <span class="nav-text">弱监督关系抽取</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#远程监督关系抽取"><span class="nav-number">1.6.7.</span> <span class="nav-text">远程监督关系抽取</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#无监督关系抽取"><span class="nav-number">1.6.8.</span> <span class="nav-text">无监督关系抽取</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#基于深度学习的关系抽取"><span class="nav-number">1.6.9.</span> <span class="nav-text">基于深度学习的关系抽取</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#词嵌入"><span class="nav-number">1.6.9.1.</span> <span class="nav-text">词嵌入</span></a></li></ol></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">DongxianGu</p>
  <div class="site-description" itemprop="description">Life,Happy,Code</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">57</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">12</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Jiangsu,Dongxian</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : 'VUd8OrYllPMWVxDqLagwIGsP-gzGzoHsz',
      appKey     : 'PiWbdIfbwtkQbPbW1Fh5WN2h',
      placeholder: "Just go go",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : '' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"log":false,"model":{"jsonPath":"/live2dw/assets/koharu.model.json"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":true},"react":{"opacity":0.7}});</script></body>
</html>
